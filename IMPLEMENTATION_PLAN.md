# ðŸŽ® Git Quest: Data Engineering Implementation Plan

## ðŸŽ¯ Project Vision
Transform Git Quest into a **full-scale Data Engineering project** that teaches Git while demonstrating real-world DE practices: ETL pipelines, data quality, orchestration, analytics, and scalable architecture.

---

## ðŸ“Š Data Engineering Architecture

### The DE Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PRESENTATION LAYER                       â”‚
â”‚  React Frontend + xterm.js + D3.js Visualizations           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                         â”‚
â”‚  FastAPI (Python) - Game Logic & Business Rules             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   PostgreSQL    â”‚  â”‚    Redis     â”‚  â”‚    DuckDB      â”‚ â”‚
â”‚  â”‚ (Game State)    â”‚  â”‚  (Sessions)  â”‚  â”‚  (Analytics)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ETL/DATA PIPELINE LAYER                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Apache Airflow / Prefect (Orchestration)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Tutorial Content Pipeline                            â”‚   â”‚
â”‚  â”‚  â€¢ Extract: Parse .txt transcripts                    â”‚   â”‚
â”‚  â”‚  â€¢ Transform: dbt models for data quality            â”‚   â”‚
â”‚  â”‚  â€¢ Load: Insert into PostgreSQL + Parquet files      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Player Analytics Pipeline                            â”‚   â”‚
â”‚  â”‚  â€¢ Stream: Real-time player events                    â”‚   â”‚
â”‚  â”‚  â€¢ Aggregate: Learning patterns & metrics            â”‚   â”‚
â”‚  â”‚  â€¢ Store: Time-series data in DuckDB                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ANALYTICS & INSIGHTS LAYER                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Metabase/     â”‚  â”‚  Jupyter     â”‚  â”‚  Great         â”‚ â”‚
â”‚  â”‚   Superset      â”‚  â”‚  Notebooks   â”‚  â”‚  Expectations  â”‚ â”‚
â”‚  â”‚  (Dashboards)   â”‚  â”‚  (Analysis)  â”‚  â”‚  (Data Quality)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ› ï¸ Technology Stack (DE-Focused)

### **Backend & Game Engine (Python)**
- **FastAPI**: REST API for game logic, player state, challenges
- **SQLAlchemy**: ORM for database interactions
- **Pydantic**: Data validation and settings management
- **GitPython**: Git operations and simulation
- **Uvicorn**: ASGI server
- **FastAPI-Users** or **python-jose**: Authentication & JWT tokens
- **passlib**: Password hashing (bcrypt)
- **python-multipart**: OAuth2 form data support

### **ETL & Data Pipeline (Python)**
- **Apache Airflow** or **Prefect**: Workflow orchestration
- **Pandas/Polars**: Data transformation (tutorial parsing)
- **dbt (data build tool)**: Data transformation with testing
- **Great Expectations**: Data quality validation
- **Parquet/Apache Arrow**: Efficient data storage

### **Data Storage**
- **PostgreSQL**: Primary database (game state, player progress, content)
- **Redis**: Session management, leaderboards, caching
- **DuckDB**: Analytics database (player metrics, learning patterns)
- **Parquet Files**: Archived analytics data
- **MinIO/S3** (Optional): Object storage for large assets

### **Analytics & Visualization**
- **Metabase** or **Apache Superset**: BI dashboards
- **Plotly/Altair**: Python-based visualizations
- **Jupyter Notebooks**: Ad-hoc analysis
- **Streamlit** (Optional): Custom analytics apps

### **Frontend (JavaScript)**
- **React** + **TypeScript**: UI framework
- **xterm.js**: Terminal emulation
- **D3.js** or **Vis.js**: Git graph visualization
- **TailwindCSS**: Styling
- **Vite**: Build tool

### **Data Quality & Testing**
- **Great Expectations**: Data validation pipelines
- **pytest**: Python unit tests
- **dbt tests**: Data transformation tests
- **Soda Core** (Optional): Alternative data quality

### **DevOps & Infrastructure**
- **Docker + Docker Compose**: Containerization
- **GitHub Actions**: CI/CD
- **Poetry**: Python dependency management
- **Pre-commit hooks**: Code quality

---

## ðŸ“ Project Structure

```
git-quest/
â”œâ”€â”€ backend/                          # Python FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                      # FastAPI routes
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py               # Authentication endpoints (login, register, token)
â”‚   â”‚   â”‚   â”œâ”€â”€ game.py               # Game session endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ lessons.py            # Tutorial endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ challenges.py         # Challenge endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ progress.py           # Player progress endpoints
â”‚   â”‚   â”‚   â””â”€â”€ analytics.py          # Analytics endpoints
â”‚   â”‚   â”œâ”€â”€ core/                     # Core game logic
â”‚   â”‚   â”‚   â”œâ”€â”€ game_engine.py        # Main game loop
â”‚   â”‚   â”‚   â”œâ”€â”€ git_simulator.py      # Git command execution
â”‚   â”‚   â”‚   â”œâ”€â”€ challenge_engine.py   # Challenge validation
â”‚   â”‚   â”‚   â”œâ”€â”€ story_engine.py       # Narrative system
â”‚   â”‚   â”‚   â””â”€â”€ security.py           # Auth utilities (JWT, password hashing)
â”‚   â”‚   â”œâ”€â”€ models/                   # SQLAlchemy models
â”‚   â”‚   â”‚   â”œâ”€â”€ user.py               # User authentication model
â”‚   â”‚   â”‚   â”œâ”€â”€ player.py
â”‚   â”‚   â”‚   â”œâ”€â”€ lesson.py
â”‚   â”‚   â”‚   â”œâ”€â”€ challenge.py
â”‚   â”‚   â”‚   â”œâ”€â”€ achievement.py
â”‚   â”‚   â”‚   â””â”€â”€ progress.py
â”‚   â”‚   â”œâ”€â”€ schemas/                  # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”‚   â””â”€â”€ db/                       # Database utilities
â”‚   â”œâ”€â”€ tests/                        # pytest tests
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ pyproject.toml                # Poetry config
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ data-pipeline/                    # ETL & Orchestration
â”‚   â”œâ”€â”€ airflow/                      # Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ tutorial_content_etl.py
â”‚   â”‚   â”‚   â”œâ”€â”€ player_analytics_pipeline.py
â”‚   â”‚   â”‚   â””â”€â”€ data_quality_checks.py
â”‚   â”‚   â””â”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ dbt/                          # dbt project
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/              # Raw data cleaning
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ stg_tutorials.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ stg_player_events.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ intermediate/         # Business logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ int_lesson_progress.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ int_command_usage.sql
â”‚   â”‚   â”‚   â””â”€â”€ marts/                # Analytics-ready tables
â”‚   â”‚   â”‚       â”œâ”€â”€ fct_player_sessions.sql
â”‚   â”‚   â”‚       â”œâ”€â”€ dim_lessons.sql
â”‚   â”‚   â”‚       â””â”€â”€ analytics_learning_patterns.sql
â”‚   â”‚   â”œâ”€â”€ tests/                    # dbt tests
â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ great_expectations/           # Data quality
â”‚   â”‚   â”œâ”€â”€ expectations/
â”‚   â”‚   â”‚   â”œâ”€â”€ tutorial_content_suite.json
â”‚   â”‚   â”‚   â””â”€â”€ player_data_suite.json
â”‚   â”‚   â””â”€â”€ checkpoints/
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ parse_tutorials.py        # Tutorial text â†’ JSON/Parquet
â”‚       â”œâ”€â”€ load_to_db.py             # Load parsed data to PostgreSQL
â”‚       â””â”€â”€ analytics_export.py       # Export to DuckDB/Parquet
â”‚
â”œâ”€â”€ frontend/                         # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Terminal/             # xterm.js integration
â”‚   â”‚   â”‚   â”œâ”€â”€ Visualizations/       # D3.js Git graphs
â”‚   â”‚   â”‚   â”œâ”€â”€ Lessons/              # Tutorial UI
â”‚   â”‚   â”‚   â”œâ”€â”€ Challenges/           # Challenge UI
â”‚   â”‚   â”‚   â””â”€â”€ Progress/             # Progress tracking UI
â”‚   â”‚   â”œâ”€â”€ hooks/                    # React hooks
â”‚   â”‚   â”œâ”€â”€ services/                 # API clients
â”‚   â”‚   â”œâ”€â”€ store/                    # State management
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ analytics/                        # Analytics & Reporting
â”‚   â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”‚   â”œâ”€â”€ player_behavior_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ learning_effectiveness.ipynb
â”‚   â”‚   â””â”€â”€ command_usage_patterns.ipynb
â”‚   â”œâ”€â”€ dashboards/                   # Metabase/Superset configs
â”‚   â””â”€â”€ reports/                      # Generated reports
â”‚
â”œâ”€â”€ content/                          # Game content (data lake)
â”‚   â”œâ”€â”€ raw/                          # Original transcripts
â”‚   â”‚   â”œâ”€â”€ Introduction/
â”‚   â”‚   â”œâ”€â”€ Intermediate/
â”‚   â”‚   â””â”€â”€ Advanced/
â”‚   â”œâ”€â”€ parsed/                       # Parsed JSON
â”‚   â”‚   â”œâ”€â”€ lessons/
â”‚   â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â””â”€â”€ scenarios/
â”‚   â”œâ”€â”€ parquet/                      # Parquet files for analytics
â”‚   â””â”€â”€ stories/                      # Narrative content
â”‚       â”œâ”€â”€ introduction_arc.json
â”‚       â”œâ”€â”€ intermediate_arc.json
â”‚       â””â”€â”€ advanced_arc.json
â”‚
â”œâ”€â”€ database/                         # Database migrations & seeds
â”‚   â”œâ”€â”€ migrations/                   # Alembic migrations
â”‚   â”œâ”€â”€ seeds/                        # Initial data
â”‚   â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ docker/                           # Docker configurations
â”‚   â”œâ”€â”€ backend.Dockerfile
â”‚   â”œâ”€â”€ frontend.Dockerfile
â”‚   â”œâ”€â”€ airflow.Dockerfile
â”‚   â””â”€â”€ docker-compose.yml            # Full stack
â”‚
â”œâ”€â”€ docs/                             # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data_model.md
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â””â”€â”€ analytics_guide.md
â”‚
â””â”€â”€ infrastructure/                   # Infrastructure as code
    â”œâ”€â”€ terraform/                    # (Optional) Cloud deployment
    â””â”€â”€ kubernetes/                   # (Optional) K8s configs
```

---

## ðŸ”„ Data Engineering Workflows

### **Workflow 1: Tutorial Content ETL Pipeline**

```python
# Airflow DAG: tutorial_content_etl.py

"""
DAG: Tutorial Content ETL
Schedule: On-demand (triggered when new tutorials added)
Purpose: Parse tutorial transcripts and load to PostgreSQL
"""

from airflow import DAG
from airflow.operators.python import PythonOperator

# Tasks:
# 1. Extract: Read .txt files from Introduction/Intermediate/Advanced
# 2. Transform:
#    - Parse timestamps and sections
#    - Extract Git commands
#    - Identify learning objectives
#    - Generate challenge prompts
#    - Run Great Expectations validation
# 3. Load:
#    - Insert into PostgreSQL (lessons, commands, concepts tables)
#    - Export to Parquet for analytics
#    - Invalidate Redis cache
```

**Data Quality Checks (Great Expectations):**
- Every lesson has 3+ sections
- All Git commands are valid syntax
- No duplicate lesson IDs
- Timestamps are in chronological order
- Every lesson has at least 1 practice scenario

---

### **Workflow 2: Player Analytics Pipeline**

```python
# Airflow DAG: player_analytics_pipeline.py

"""
DAG: Player Analytics ETL
Schedule: Every 15 minutes
Purpose: Process player events and generate learning insights
"""

# Tasks:
# 1. Extract: Read player events from PostgreSQL (commands_executed, challenges_completed)
# 2. Transform (dbt models):
#    - Aggregate session duration
#    - Calculate command success rates
#    - Identify struggling concepts
#    - Generate learning velocity metrics
# 3. Load:
#    - Insert into DuckDB analytics database
#    - Update Metabase dashboards
```

**dbt Models:**
```sql
-- models/marts/fct_player_sessions.sql
-- Fact table: Player learning sessions

SELECT
    session_id,
    player_id,
    lesson_id,
    started_at,
    completed_at,
    duration_seconds,
    commands_executed,
    errors_encountered,
    hints_used,
    success_rate
FROM {{ ref('int_session_metrics') }}
```

---

### **Workflow 3: Data Quality Monitoring**

```python
# Airflow DAG: data_quality_checks.py

"""
DAG: Data Quality Monitoring
Schedule: Daily at 2 AM
Purpose: Validate data integrity across all systems
"""

# Tasks:
# 1. Great Expectations checkpoints:
#    - Validate tutorial content schema
#    - Check player data completeness
#    - Validate Git command references
# 2. dbt tests:
#    - Referential integrity
#    - Uniqueness constraints
#    - Not-null checks
# 3. Alerting:
#    - Send Slack notification on failures
```

---

## ðŸ—„ï¸ Database Schema (PostgreSQL)

### Core Tables

```sql
-- Users (Authentication)
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    username VARCHAR(50) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    is_verified BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP
);

-- Password Reset Tokens
CREATE TABLE password_reset_tokens (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    token VARCHAR(255) UNIQUE NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Refresh Tokens (for JWT)
CREATE TABLE refresh_tokens (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    token VARCHAR(255) UNIQUE NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    revoked BOOLEAN DEFAULT FALSE
);

-- Players (Game Profile)
CREATE TABLE players (
    id UUID PRIMARY KEY,
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    username VARCHAR(50) UNIQUE NOT NULL,
    display_name VARCHAR(100),
    avatar_url VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW(),
    current_level VARCHAR(20),
    total_xp INTEGER DEFAULT 0,
    UNIQUE(user_id)
);

-- Lessons
CREATE TABLE lessons (
    id VARCHAR(50) PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    level VARCHAR(20) NOT NULL,  -- 'introduction', 'intermediate', 'advanced'
    order_index INTEGER,
    story_hook TEXT,
    content JSONB,  -- Structured lesson content
    unlocks JSONB,  -- Prerequisites and unlocks
    created_at TIMESTAMP DEFAULT NOW()
);

-- Git Commands Reference
CREATE TABLE git_commands (
    id VARCHAR(50) PRIMARY KEY,
    command VARCHAR(100) NOT NULL,
    syntax TEXT,
    description TEXT,
    category VARCHAR(50),
    difficulty INTEGER,
    examples JSONB,
    common_mistakes JSONB
);

-- Player Progress
CREATE TABLE player_progress (
    id UUID PRIMARY KEY,
    player_id UUID REFERENCES players(id),
    lesson_id VARCHAR(50) REFERENCES lessons(id),
    status VARCHAR(20),  -- 'not_started', 'in_progress', 'completed'
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    time_spent_seconds INTEGER,
    score INTEGER,
    UNIQUE(player_id, lesson_id)
);

-- Player Events (for analytics)
CREATE TABLE player_events (
    id BIGSERIAL PRIMARY KEY,
    player_id UUID REFERENCES players(id),
    event_type VARCHAR(50),  -- 'command_executed', 'challenge_completed', etc.
    event_data JSONB,
    timestamp TIMESTAMP DEFAULT NOW()
);

-- Challenges
CREATE TABLE challenges (
    id VARCHAR(50) PRIMARY KEY,
    lesson_id VARCHAR(50) REFERENCES lessons(id),
    title VARCHAR(255),
    type VARCHAR(50),  -- 'crisis', 'command_mastery', 'quiz', 'speed_run', 'boss'
    difficulty INTEGER,
    scenario TEXT,
    success_criteria JSONB,
    hints JSONB
);

-- Challenge Attempts
CREATE TABLE challenge_attempts (
    id UUID PRIMARY KEY,
    player_id UUID REFERENCES players(id),
    challenge_id VARCHAR(50) REFERENCES challenges(id),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    success BOOLEAN,
    commands_used JSONB,
    score INTEGER
);

-- Achievements
CREATE TABLE achievements (
    id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(100),
    description TEXT,
    badge_icon VARCHAR(255),
    unlock_criteria JSONB
);

-- Player Achievements
CREATE TABLE player_achievements (
    player_id UUID REFERENCES players(id),
    achievement_id VARCHAR(50) REFERENCES achievements(id),
    unlocked_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (player_id, achievement_id)
);
```

---

## ðŸ“Š Analytics Database (DuckDB)

**Why DuckDB?**
- Fast OLAP queries for analytics
- Embedded (no separate server)
- Excellent Parquet integration
- SQL interface for analysts

**Analytics Tables:**
```sql
-- Player Learning Patterns
CREATE TABLE analytics.learning_patterns AS
SELECT
    player_id,
    lesson_id,
    AVG(time_spent_seconds) as avg_time,
    COUNT(*) as attempts,
    AVG(score) as avg_score,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY time_spent_seconds) as median_time
FROM player_progress
GROUP BY player_id, lesson_id;

-- Command Usage Statistics
CREATE TABLE analytics.command_usage AS
SELECT
    event_data->>'command' as command,
    event_data->>'lesson_id' as lesson_id,
    COUNT(*) as usage_count,
    AVG(CASE WHEN event_data->>'success' = 'true' THEN 1 ELSE 0 END) as success_rate
FROM player_events
WHERE event_type = 'command_executed'
GROUP BY command, lesson_id;
```

---

## ðŸŽ¯ 8-Phase Implementation Plan (DE Edition)

### **PHASE 1: Data Foundation & ETL Pipeline** (Week 1-2)

**Objective:** Set up DE infrastructure and build tutorial content pipeline

**Tasks:**

1. **Infrastructure Setup**
   - [ ] Set up PostgreSQL database
   - [ ] Set up Redis for caching
   - [ ] Set up DuckDB for analytics
   - [ ] Docker Compose for local development
   - [ ] Initialize Alembic for migrations

2. **Tutorial Content ETL Pipeline**
   - [ ] Build tutorial parser (Pandas/Polars)
     ```python
     # scripts/parse_tutorials.py
     def parse_tutorial_file(filepath):
         """Extract sections, commands, timestamps from .txt"""
         # Parse timestamp sections
         # Extract Git commands with regex
         # Identify learning objectives
         # Generate structured JSON
     ```
   - [ ] Set up Great Expectations validation
   - [ ] Create dbt staging models
   - [ ] Load to PostgreSQL

3. **Airflow Setup**
   - [ ] Initialize Airflow with Docker
   - [ ] Create `tutorial_content_etl` DAG
   - [ ] Set up data quality DAG
   - [ ] Configure connections (PostgreSQL, S3/MinIO)

**Deliverables:**
- [ ] Running PostgreSQL + Redis + DuckDB stack
- [ ] Tutorial parser script (25 tutorials â†’ structured data)
- [ ] Airflow DAG for content pipeline
- [ ] Great Expectations test suite
- [ ] Database schema migration scripts

---

### **PHASE 2: Backend API & Game Engine** (Week 2-4)

**Objective:** Build FastAPI backend with game logic

**Tasks:**

1. **FastAPI Application**
   - [ ] Project structure with Poetry
   - [ ] SQLAlchemy models (User, Player, Lesson, Challenge, etc.)
   - [ ] Pydantic schemas for validation
   - [ ] CRUD operations for all models

2. **Authentication & Security**
   - [ ] JWT token generation and validation (python-jose)
   - [ ] Password hashing with bcrypt (passlib)
   - [ ] OAuth2 password flow implementation
   - [ ] User registration and login endpoints
   - [ ] Token refresh mechanism
   - [ ] Password reset workflow (email optional for MVP)
   - [ ] Protected route dependencies (get_current_user)
   - [ ] Role-based access control (optional)

3. **Core Game Engine (Python)**
   - [ ] Game state manager
   - [ ] Lesson progression logic
   - [ ] Git command simulator (GitPython)
   - [ ] Challenge validation engine
   - [ ] Achievement unlock system

4. **API Endpoints**
   ```python
   # app/api/auth.py
   @router.post("/auth/register")
   async def register(user_data: UserCreate)

   @router.post("/auth/login")
   async def login(credentials: OAuth2PasswordRequestForm)

   @router.post("/auth/refresh")
   async def refresh_token(refresh_token: str)

   @router.post("/auth/logout")
   async def logout(current_user: User = Depends(get_current_user))

   @router.post("/auth/forgot-password")
   async def forgot_password(email: str)

   @router.post("/auth/reset-password")
   async def reset_password(token: str, new_password: str)

   @router.get("/auth/me")
   async def get_current_user_info(current_user: User = Depends(get_current_user))

   # app/api/lessons.py
   @router.get("/lessons")
   async def get_lessons(level: str = None, current_user: User = Depends(get_current_user))

   @router.get("/lessons/{lesson_id}")
   async def get_lesson(lesson_id: str, current_user: User = Depends(get_current_user))

   @router.post("/lessons/{lesson_id}/start")
   async def start_lesson(lesson_id: str, current_user: User = Depends(get_current_user))

   # app/api/game.py
   @router.post("/execute-command")
   async def execute_git_command(command: str, session_id: UUID, current_user: User = Depends(get_current_user))

   # app/api/challenges.py
   @router.post("/challenges/{challenge_id}/submit")
   async def submit_challenge(challenge_id: str, solution: dict, current_user: User = Depends(get_current_user))
   ```

5. **Event Tracking**
   - [ ] Player event logging (commands, completions, errors)
   - [ ] Real-time event streaming to PostgreSQL
   - [ ] Redis session management

**Deliverables:**
- [ ] FastAPI application with 30+ endpoints (including auth)
- [ ] JWT authentication system with refresh tokens
- [ ] User registration and login flow
- [ ] Git command simulator
- [ ] Game engine core logic
- [ ] pytest test suite (>80% coverage)

---

### **PHASE 3: Story System & Content Integration** (Week 4-5)

**Objective:** Build narrative engine and integrate story content

**Tasks:**

1. **Story Content Creation**
   - [ ] Write Introduction Arc (7 chapters)
   - [ ] Write Intermediate Arc (7 chapters)
   - [ ] Write Advanced Arc (11 chapters)
   - [ ] Character profiles and dialogue

2. **Story Engine (Python)**
   - [ ] Story state machine
   - [ ] Dialogue system
   - [ ] Choice/branching logic
   - [ ] Character companion system

3. **Content Integration**
   - [ ] Map stories to lessons (1:1)
   - [ ] Inject story hooks into API responses
   - [ ] Create story progression tracking

**Deliverables:**
- [ ] 25 story chapters (JSON format)
- [ ] Story engine implementation
- [ ] Character companion system
- [ ] Story API endpoints

---

### **PHASE 4: Frontend Development** (Week 5-7)

**Objective:** Build React frontend with terminal and visualizations

**Tasks:**

1. **React Application Setup**
   - [ ] Vite + TypeScript + TailwindCSS
   - [ ] React Router for navigation
   - [ ] State management (Zustand/Redux)
   - [ ] API client (Axios/Fetch) with JWT interceptors

2. **Authentication UI**
   - [ ] Login page/modal
   - [ ] Registration page/modal
   - [ ] Password reset flow
   - [ ] Protected route wrapper
   - [ ] Auth context/state management
   - [ ] Token refresh handling
   - [ ] Logout functionality
   - [ ] User profile dropdown

3. **Core Components**
   - [ ] Terminal component (xterm.js)
   - [ ] Lesson viewer
   - [ ] Challenge interface
   - [ ] Progress dashboard

4. **Git Visualization (D3.js)**
   - [ ] Commit graph renderer
   - [ ] Branch diagram
   - [ ] Interactive repository explorer
   - [ ] Before/after state comparison

5. **Game UI**
   - [ ] Story dialogue system
   - [ ] Achievement notifications
   - [ ] XP/level progress bars
   - [ ] Leaderboard

**Deliverables:**
- [ ] Fully functional React frontend
- [ ] Complete authentication UI (login, register, password reset)
- [ ] JWT token management with auto-refresh
- [ ] Terminal emulator integration
- [ ] Git visualization components
- [ ] Responsive design

---

### **PHASE 5: Challenge System & Gamification** (Week 7-8)

**Objective:** Implement challenges, achievements, and game mechanics

**Tasks:**

1. **Challenge Types Implementation**
   - [ ] Crisis scenarios (15 challenges)
   - [ ] Command mastery duels (10 challenges)
   - [ ] Conceptual quizzes (15 challenges)
   - [ ] Speed runs (5 challenges)
   - [ ] Boss battles (3 end-of-level challenges)

2. **Challenge Engine**
   - [ ] Solution validation logic
   - [ ] Scoring algorithm
   - [ ] Hint system
   - [ ] Time tracking

3. **Achievement System**
   - [ ] Define 50+ achievements
   - [ ] Unlock criteria validation
   - [ ] Badge artwork/icons
   - [ ] Achievement notification system

4. **Progression System**
   - [ ] XP calculation
   - [ ] Level progression
   - [ ] Skill tree (command unlocks)
   - [ ] Leaderboard logic (Redis)

**Deliverables:**
- [ ] 50+ challenges across all types
- [ ] Challenge validation engine
- [ ] Achievement system
- [ ] Leaderboard API

---

### **PHASE 6: Analytics Pipeline & Dashboards** (Week 8-9)

**Objective:** Build analytics infrastructure for learning insights

**Tasks:**

1. **dbt Transformation Models**
   - [ ] Staging models (raw data cleaning)
   - [ ] Intermediate models (business logic)
   - [ ] Mart models (analytics-ready)
   - [ ] dbt tests for data quality

2. **Player Analytics Pipeline**
   - [ ] Airflow DAG for analytics ETL
   - [ ] DuckDB aggregation queries
   - [ ] Parquet export for archival
   - [ ] Learning effectiveness metrics

3. **Analytics Dashboards (Metabase/Superset)**
   - [ ] Player engagement dashboard
   - [ ] Learning effectiveness metrics
   - [ ] Command usage patterns
   - [ ] Challenge difficulty analysis
   - [ ] Funnel analysis (lesson completion)

4. **Jupyter Notebooks**
   - [ ] Player behavior analysis
   - [ ] Learning pattern detection
   - [ ] Recommendation algorithm (struggling concepts)

**Deliverables:**
- [ ] dbt project with 15+ models
- [ ] Player analytics pipeline
- [ ] 5+ analytics dashboards
- [ ] Jupyter notebooks for analysis

---

### **PHASE 7: Advanced Features & Polish** (Week 9-10)

**Objective:** Add advanced visualizations and UX polish

**Tasks:**

1. **Repository Visualization Lab**
   - [ ] Interactive 3D Git graph (or advanced 2D)
   - [ ] Time-travel slider
   - [ ] HEAD cursor visualization
   - [ ] Explore mode

2. **Git Crimes Comedy Series**
   - [ ] Write 25+ comedic scenarios
   - [ ] ASCII art for each "crime"
   - [ ] Animation system
   - [ ] Unlock after each lesson

3. **UX Enhancements**
   - [ ] Loading states and skeletons
   - [ ] Error handling and messages
   - [ ] Accessibility (WCAG 2.1 AA)
   - [ ] Mobile responsiveness
   - [ ] Dark/light themes

4. **Performance Optimization**
   - [ ] Frontend code splitting
   - [ ] API response caching (Redis)
   - [ ] Database query optimization
   - [ ] Lazy loading for content

**Deliverables:**
- [ ] Advanced repository visualization
- [ ] Git Crimes series (25+ episodes)
- [ ] Polished UX
- [ ] Performance benchmarks

---

### **PHASE 8: Testing, Documentation & Deployment** (Week 10-12)

**Objective:** Production readiness and launch

**Tasks:**

1. **Testing**
   - [ ] Backend: pytest (>80% coverage)
   - [ ] Frontend: Jest + React Testing Library
   - [ ] E2E: Playwright/Cypress
   - [ ] Security testing: Authentication flows, SQL injection, XSS
   - [ ] Load testing: Locust (API performance)
   - [ ] Data quality: Great Expectations

2. **Documentation**
   - [ ] API documentation (Swagger/OpenAPI)
   - [ ] Architecture documentation
   - [ ] Data model documentation
   - [ ] Analytics guide
   - [ ] User guide
   - [ ] Contributor guide

3. **Deployment**
   - [ ] Docker Compose for production
   - [ ] GitHub Actions CI/CD pipeline
   - [ ] Environment variables management (.env, secrets)
   - [ ] HTTPS/SSL certificate setup
   - [ ] CORS configuration
   - [ ] Rate limiting (API protection)
   - [ ] Cloud deployment (AWS/GCP/Azure or VPS)
   - [ ] Monitoring (Prometheus + Grafana)
   - [ ] Logging (ELK stack or CloudWatch)

4. **Launch Materials**
   - [ ] README with screenshots
   - [ ] Demo video
   - [ ] Blog post
   - [ ] Social media assets

**Deliverables:**
- [ ] Full test suite
- [ ] Complete documentation
- [ ] Deployed application
- [ ] Monitoring dashboards
- [ ] Launch materials

---

## ðŸ”’ Security Considerations

### Authentication & Authorization

1. **Password Security**
   - Bcrypt hashing with salt (min 12 rounds)
   - Password strength requirements (min 8 chars, complexity)
   - Account lockout after failed attempts
   - Secure password reset with time-limited tokens

2. **JWT Token Security**
   - Short-lived access tokens (15-30 minutes)
   - Longer-lived refresh tokens (7 days, stored securely)
   - Token rotation on refresh
   - Blacklist/revoke tokens on logout
   - HttpOnly cookies for token storage (frontend)

3. **Session Management**
   - Redis-based session storage
   - Session timeout and cleanup
   - Concurrent session limits
   - Device/IP tracking for suspicious activity

### API Security

1. **Input Validation**
   - Pydantic schemas for all inputs
   - SQL injection prevention (SQLAlchemy ORM)
   - XSS prevention (sanitize outputs)
   - Command injection protection (Git simulator sandboxing)

2. **Rate Limiting**
   - API endpoint rate limits (per IP/user)
   - Sliding window algorithm
   - DDoS protection
   - Exponential backoff for repeated failures

3. **CORS & Headers**
   - Strict CORS policy (whitelist origins)
   - Security headers (CSP, X-Frame-Options, etc.)
   - HTTPS enforcement
   - Secure cookie flags (HttpOnly, Secure, SameSite)

### Data Security

1. **Database Security**
   - Parameterized queries (prevent SQL injection)
   - Principle of least privilege (DB user permissions)
   - Encrypted connections (SSL/TLS)
   - Regular backups with encryption

2. **PII Protection**
   - Email encryption at rest (optional)
   - GDPR compliance (data deletion, export)
   - Anonymized analytics data
   - No logging of sensitive data

3. **Git Simulator Security**
   - Sandboxed execution environment
   - No access to host filesystem
   - Command whitelist (only allow safe Git commands)
   - Resource limits (CPU, memory, disk)

### Infrastructure Security

1. **Environment**
   - Environment variables for secrets
   - No hardcoded credentials
   - Secret rotation policy
   - Vault/secrets manager (production)

2. **Container Security**
   - Minimal base images (Alpine)
   - Non-root user in containers
   - Image vulnerability scanning
   - Regular dependency updates

3. **Monitoring & Auditing**
   - Authentication event logging
   - Failed login attempt tracking
   - Admin action audit trail
   - Anomaly detection alerts

---

## ðŸ“ˆ Data Engineering Highlights

### What Makes This a Real DE Project?

1. **ETL Pipelines**: Transform tutorial transcripts â†’ structured data
2. **Data Orchestration**: Airflow DAGs for content and analytics
3. **Data Quality**: Great Expectations + dbt tests
4. **Data Modeling**: Star schema (facts + dimensions)
5. **Analytics Engineering**: dbt models for business logic
6. **OLAP Database**: DuckDB for fast analytics
7. **Real-time Events**: Player event streaming
8. **BI Dashboards**: Metabase/Superset for insights
9. **Data Warehousing**: Multi-database architecture
10. **Performance**: Parquet, columnar storage, caching

### Learning Outcomes for You

By building Git Quest, you'll practice:
- Building production ETL pipelines
- Data modeling (OLTP vs OLAP)
- Analytics engineering with dbt
- Data quality frameworks
- Event-driven architecture
- API design for data products
- Dashboard development
- Data pipeline orchestration
- Multi-database architecture
- DevOps for data systems

---

## ðŸš€ Next Steps

**Let's Start with Phase 1!**

I recommend we begin with:

1. **Tutorial Parser Script** - Extract data from the 25 .txt files
2. **Database Schema** - PostgreSQL setup with migrations
3. **Great Expectations** - Data quality for tutorial content
4. **First Airflow DAG** - Tutorial content ETL
5. **Complete staging_and_committing_files.txt** - Fill missing content

**Shall we start building the tutorial parser and database schema now?**
